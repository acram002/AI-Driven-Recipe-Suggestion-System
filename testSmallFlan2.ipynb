{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acram002/AI-Driven-Recipe-Suggestion-System/blob/main/testSmallFlan2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb7UF5uHqZbk"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 3: Load and preprocess dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/full_dataset.csv')\n",
        "df = df[['NER', 'ingredients', 'directions']].dropna() # added ingredients (to model response)\n",
        "df = df.sample(n=30000, random_state=42) # increased from 5 to 10 to 30k\n",
        "\n",
        "\n",
        "# Fix stringified list formats and clean ingredient quantities\n",
        "def clean_ingredient_list(ingredient_str):\n",
        "    try:\n",
        "        ingredients = ast.literal_eval(ingredient_str) if isinstance(ingredient_str, str) else ingredient_str\n",
        "        cleaned = []\n",
        "        for item in ingredients:\n",
        "            if any(unit in item.lower() for unit in ['tortilla', 'egg', 'clove', 'slice']):\n",
        "                parts = item.split()\n",
        "                if len(parts) > 1 and parts[0][0].isdigit():\n",
        "                    cleaned.append(\" \".join(parts[1:]))\n",
        "                else:\n",
        "                    cleaned.append(item)\n",
        "            elif any(unit in item.lower() for unit in ['lb', 'pound', 'lbs', 'kg']):\n",
        "                cleaned.append(item.replace(\"6 lbs\", \"1.5 lbs\").replace(\"5 lbs\", \"1.5 lbs\"))\n",
        "            else:\n",
        "                cleaned.append(item)\n",
        "        return cleaned\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "df['NER'] = df['NER'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
        "df['ingredients'] = df['ingredients'].apply(clean_ingredient_list)\n",
        "\n",
        "df['response'] = df.apply(\n",
        "    lambda row: f\"Ingredients:\\n{'\\n'.join(row['ingredients'])}\\n\\nInstructions:\\n\" +\n",
        "                \"\\n\".join(ast.literal_eval(row['directions']))\n",
        "    if isinstance(row['directions'], str) and row['directions'].startswith(\"[\") else str(row['directions']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['prompt'] = df['NER'].apply(lambda x: f\"Generate a recipe:\\nIngredients: {', '.join(x) if isinstance(x, list) else str(x)}\")\n",
        "\n",
        "# Fix stringified list formats\n",
        "#df['NER'] = df['NER'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
        "#df['response'] = df.apply(\n",
        "#    lambda row: f\"Ingredients:\\n{row['ingredients'].strip()}\\n\\nInstructions:\\n\" + \"\\n\".join(ast.literal_eval(row['directions']))\n",
        "#    if isinstance(row['directions'], str) and row['directions'].startswith(\"[\") else str(row['directions']),\n",
        "#    axis=1\n",
        "#) # adding ingredients (quantities) to model response\n",
        "#df['response'] = df['directions'].apply(lambda x: \"\\n\".join(ast.literal_eval(x)) if isinstance(x, str) and x.startswith(\"[\") else str(x))\n",
        "#df['prompt'] = df['NER'].apply(lambda x: f\"Generate a recipe:\\nIngredients: {', '.join(x) if isinstance(x, list) else str(x)}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"ðŸ”Ž FIRST ROW PROMPT:\\n\", df.iloc[0]['prompt'])\n",
        "print(\"ðŸ”Ž FIRST ROW RESPONSE:\\n\", df.iloc[0]['response'])\n",
        "\n",
        "# STEP 4: Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df[['prompt', 'response']])\n",
        "\n",
        "# STEP 5: Tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_length = 512\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(example['prompt'], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    labels = tokenizer(example['response'], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    labels[\"input_ids\"] = [\n",
        "        (token if token != tokenizer.pad_token_id else -100)\n",
        "        for token in labels[\"input_ids\"]\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized = dataset.map(preprocess)\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# STEP 6: Confirm valid labels\n",
        "sample = tokenized[0]\n",
        "non_masked = sum([1 for token in sample[\"labels\"] if token != -100])\n",
        "print(f\"\\nâœ… Non-masked label tokens: {non_masked} out of {len(sample['labels'])}\")\n",
        "print(\"ðŸŽ¯ Decoded target:\\n\", tokenizer.decode([t for t in sample[\"labels\"] if t != -100]))\n",
        "\n",
        "# STEP 7: Manual PyTorch Training Loop\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from torch.optim import AdamW  # âœ… Correct import for newer transformers\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model and move to GPU\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"\\nðŸ“¦ Model loaded to: {device}\")\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(tokenized, batch_size=2, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(1):  # 1 epoch\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# STEP 8: Save model to Drive\n",
        "save_path = \"/content/drive/MyDrive/flan_recipe_manual_final3\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"\\nâœ… Model and tokenizer saved to: {save_path}\")\n",
        "\n",
        "\n",
        "#increase training sample size\n",
        "#maybe jump to flan large ? or other model? maybe only when training script is ready\n",
        "# increase temperature for output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prompt the user for ingredients\n",
        "user_input = input(\"Enter ingredients (comma-separated):\\n\")\n",
        "ingredients = [i.strip() for i in user_input.split(\",\")]\n",
        "prompt = f\"Generate a recipe:\\nIngredients: {', '.join(ingredients)}\"\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "    **inputs,\n",
        "    max_length=512,# was 256 havent tested new\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.2,# was 0.8 havent tested new\n",
        "    repetition_penalty=1.2,  # ðŸ”¥ NEW: discourages repetition\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "# Print the recipe\n",
        "print(\"\\n=== Generated Recipe ===\\n\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "1nIAxSyEthbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# STEP 1: Load model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/flan_recipe_manual_final3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
        "\n",
        "# STEP 2: Define test cases\n",
        "test_prompts = [\n",
        "    [\"chicken\", \"rice\", \"broccoli\", \"soy sauce\", \"garlic\"],\n",
        "    [\"ice cream\", \"banana\", \"chocolate syrup\", \"whipped cream\"],\n",
        "    [\"eggs\", \"cheese\", \"spinach\", \"tomato\", \"tortilla\"],\n",
        "    [\"beef\", \"potatoes\", \"carrots\", \"onion\", \"broth\", \"bay leaf\"],\n",
        "    [\"tofu\", \"mushrooms\", \"soy sauce\", \"garlic\", \"sesame oil\"],\n",
        "    [\"chicken\"],  # Minimal input\n",
        "    [\"ground beef\", \"tomato sauce\", \"spaghetti\"],\n",
        "    [\"ground beef\", \"tomato sauce\", \"tortillas\"],\n",
        "]\n",
        "\n",
        "# STEP 3: Generate and print recipes\n",
        "for idx, ingredients in enumerate(test_prompts):\n",
        "    print(f\"\\nðŸ§ª Test Case {idx + 1}: {', '.join(ingredients)}\\n\")\n",
        "\n",
        "    prompt = f\"Generate a recipe:\\nIngredients: {', '.join(ingredients)}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "          **inputs,\n",
        "          do_sample=True,  # ðŸ”¥ Enable actual sampling!\n",
        "          max_length=350,\n",
        "          temperature=1.0,\n",
        "          top_k=50,\n",
        "          top_p=0.95,\n",
        "          repetition_penalty=1.2,\n",
        "          num_beams=1\n",
        ")\n",
        "\n",
        "\n",
        "    recipe = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(\"=== Generated Recipe ===\")\n",
        "    print(recipe)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "id": "kHTFAhC9wB6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}