{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN2MflIlffCAVwnASHnOkYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acram002/AI-Driven-Recipe-Suggestion-System/blob/main/Copy_of_testSmallFlanColabPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ65aqR6eLCd"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 3: Load and sample your dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/full_dataset.csv')\n",
        "df = df[['NER', 'directions']].dropna()\n",
        "df = df.sample(n=5000, random_state=42)  # Reduced for fast training\n",
        "\n",
        "# STEP 4: Format for training\n",
        "df['prompt'] = 'Generate a recipe:\\nIngredients: ' + df['NER']\n",
        "df['response'] = df['directions']\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df[['prompt', 'response']])\n",
        "\n",
        "# STEP 5: Tokenize the data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "def tokenize(example):\n",
        "    inputs = tokenizer(example['prompt'], truncation=True, padding='max_length', max_length=max_length)\n",
        "    targets = tokenizer(example['response'], truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "    # Important: mask pad tokens in labels to -100 so they're ignored in loss\n",
        "    targets['input_ids'] = [\n",
        "        (token if token != tokenizer.pad_token_id else -100)\n",
        "        for token in targets['input_ids']\n",
        "    ]\n",
        "\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n",
        "\n",
        "# STEP 6: Load model\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# STEP 7: Set up training arguments\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/flan_recipe_model_output\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# STEP 8: Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# STEP 9: Train!\n",
        "trainer.train()\n",
        "\n",
        "# STEP 10: Save model\n",
        "save_path = \"/content/drive/MyDrive/flan_recipe_model_final\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"âœ… Model and tokenizer saved to: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 3: Load and preprocess dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/full_dataset.csv')\n",
        "df = df[['NER', 'directions']].dropna()\n",
        "df = df.sample(n=5000, random_state=42)\n",
        "\n",
        "# Fix stringified list formats\n",
        "df['NER'] = df['NER'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
        "df['response'] = df['directions'].apply(lambda x: \"\\n\".join(ast.literal_eval(x)) if isinstance(x, str) and x.startswith(\"[\") else str(x))\n",
        "df['prompt'] = df['NER'].apply(lambda x: f\"Generate a recipe:\\nIngredients: {', '.join(x) if isinstance(x, list) else str(x)}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"ðŸ”Ž FIRST ROW PROMPT:\\n\", df.iloc[0]['prompt'])\n",
        "print(\"ðŸ”Ž FIRST ROW RESPONSE:\\n\", df.iloc[0]['response'])\n",
        "\n",
        "# STEP 4: Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df[['prompt', 'response']])\n",
        "\n",
        "# STEP 5: Tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_length = 512\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(example['prompt'], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    labels = tokenizer(example['response'], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    labels[\"input_ids\"] = [\n",
        "        (token if token != tokenizer.pad_token_id else -100)\n",
        "        for token in labels[\"input_ids\"]\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized = dataset.map(preprocess)\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# STEP 6: Confirm valid labels\n",
        "sample = tokenized[0]\n",
        "non_masked = sum([1 for token in sample[\"labels\"] if token != -100])\n",
        "print(f\"\\nâœ… Non-masked label tokens: {non_masked} out of {len(sample['labels'])}\")\n",
        "print(\"ðŸŽ¯ Decoded target:\\n\", tokenizer.decode([t for t in sample[\"labels\"] if t != -100]))\n",
        "\n",
        "# STEP 7: Manual PyTorch Training Loop\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSeq2SeqLM, AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model and move to GPU\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"\\nðŸ“¦ Model loaded to: {device}\")\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(tokenized, batch_size=2, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(1):  # 1 epoch\n",
        "    loop = tqdm(train_loader, desc=\"Epoch 1\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# STEP 8: Save model to Drive\n",
        "save_path = \"/content/drive/MyDrive/flan_recipe_manual_final\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"\\nâœ… Model and tokenizer saved to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "2S9ktF0WeVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user for ingredients\n",
        "user_input = input(\"Enter ingredients (comma-separated):\\n\")\n",
        "ingredients = [i.strip() for i in user_input.split(\",\")]\n",
        "prompt = f\"Generate a recipe:\\nIngredients: {', '.join(ingredients)}\"\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        num_beams=4,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8\n",
        "    )\n",
        "\n",
        "# Print the recipe\n",
        "print(\"\\n=== Generated Recipe ===\\n\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "IXPYj6kgncjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 3: Load and preprocess dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/full_dataset.csv')\n",
        "df = df[['NER', 'directions']].dropna()\n",
        "df = df.sample(n=5000, random_state=42)\n",
        "\n",
        "# ðŸ§¼ Fix stringified lists in NER and directions\n",
        "df['NER'] = df['NER'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
        "df['response'] = df['directions'].apply(lambda x: \"\\n\".join(ast.literal_eval(x)) if isinstance(x, str) and x.startswith(\"[\") else str(x))\n",
        "\n",
        "# ðŸ§  Format prompt using NER\n",
        "df['prompt'] = df['NER'].apply(lambda x: f\"Generate a recipe:\\nIngredients: {', '.join(x) if isinstance(x, list) else str(x)}\")\n",
        "\n",
        "# Print prompt and response example\n",
        "print(\"ðŸ”Ž FIRST ROW PROMPT:\\n\", df.iloc[0]['prompt'])\n",
        "print(\"ðŸ”Ž FIRST ROW RESPONSE:\\n\",\n"
      ],
      "metadata": {
        "id": "fuCpZEBQhDIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Generate a recipe:\\nIngredients: [\"chicken\", \"rice\", \"broccoli\", \"garlic\", \"soy sauce\"]'\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=256)\n",
        "\n",
        "print(\"\\n=== Generated Recipe ===\\n\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "Qah2-RoEhKBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}