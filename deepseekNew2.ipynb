{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOX7WkI1z/wh+MjK1vo0wTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acram002/AI-Driven-Recipe-Suggestion-System/blob/main/deepseekNew2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiSC_5dtYoZY"
      },
      "outputs": [],
      "source": [
        "# TRAINING BLOCK\n",
        "# ✅ STEP 1: Install dependencies\n",
        "!pip install -q unsloth datasets bitsandbytes accelerate peft\n",
        "\n",
        "# ✅ STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 3: Load model with Unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=1024,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ✅ STEP 4: Apply LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# ✅ STEP 5: Load and clean dataset\n",
        "import pandas as pd, ast\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/full_dataset.csv')\n",
        "df = df[['NER', 'ingredients', 'directions']].dropna()\n",
        "df = df.sample(n=30000, random_state=42)\n",
        "\n",
        "# Clean functions\n",
        "def clean_ingredient_list(ingredient_str):\n",
        "    try:\n",
        "        ingredients = ast.literal_eval(ingredient_str) if isinstance(ingredient_str, str) else ingredient_str\n",
        "        cleaned = []\n",
        "        for item in ingredients:\n",
        "            if any(unit in item.lower() for unit in ['tortilla', 'egg', 'clove', 'slice']):\n",
        "                parts = item.split()\n",
        "                if len(parts) > 1 and parts[0][0].isdigit():\n",
        "                    cleaned.append(\" \".join(parts[1:]))\n",
        "                else:\n",
        "                    cleaned.append(item)\n",
        "            elif any(unit in item.lower() for unit in ['lb', 'pound', 'lbs', 'kg']):\n",
        "                cleaned.append(item.replace(\"6 lbs\", \"1.5 lbs\").replace(\"5 lbs\", \"1.5 lbs\"))\n",
        "            else:\n",
        "                cleaned.append(item)\n",
        "        return cleaned\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def clean_response(text):\n",
        "    cutoff_phrases = [\n",
        "        \"Note:\", \"Nutritional\", \"Serves\", \"Source:\", \"Enjoy!\", \"This is a vegan dish\",\n",
        "        \"From Food Network\", \"submitted by\", \"adapted from\", \"let me know\", \"Thanks for visiting\"\n",
        "    ]\n",
        "    for phrase in cutoff_phrases:\n",
        "        idx = text.lower().find(phrase.lower())\n",
        "        if idx != -1:\n",
        "            return text[:idx].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning\n",
        "df['NER'] = df['NER'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
        "df['ingredients'] = df['ingredients'].apply(clean_ingredient_list)\n",
        "df['response'] = df.apply(\n",
        "    lambda row: f\"Ingredients:\\n{chr(10).join(row['ingredients'])}\\n\\nInstructions:\\n\" + \"\\n\".join(ast.literal_eval(row['directions']))\n",
        "    if isinstance(row['directions'], str) and row['directions'].startswith(\"[\") else str(row['directions']),\n",
        "    axis=1\n",
        ")\n",
        "df['response'] = df['response'].apply(clean_response)\n",
        "df = df[df['response'].str.len() > 100]\n",
        "df = df[df['NER'].apply(lambda x: isinstance(x, list) and len(x) >= 3)]\n",
        "\n",
        "# Create prompt-response and convert to dataset\n",
        "df['prompt'] = df['NER'].apply(\n",
        "    lambda x: f\"<|user|> Generate a recipe using ONLY the following ingredients: {', '.join(x)}. Return only the title, ingredients, and step-by-step instructions. <|assistant|>\"\n",
        ")\n",
        "df['text'] = df['prompt'] + \"\\n\" + df['response']\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "dataset = dataset.train_test_split(test_size=0.05)\n",
        "train_data = dataset[\"train\"]\n",
        "eval_data = dataset[\"test\"]\n",
        "\n",
        "# ✅ STEP 6: Tokenization\n",
        "max_length = 1024\n",
        "\n",
        "def tokenize(example):\n",
        "    model_inputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(token if token != tokenizer.pad_token_id else -100) for token in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_data.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_eval = eval_data.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ✅ STEP 7: Setup Trainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/deepseek-7b-recipe-lora2\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    warmup_steps=5,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    learning_rate=2e-4,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# ✅ STEP 8: Train\n",
        "trainer.train()\n",
        "\n",
        "# ✅ STEP 9: Save\n",
        "model.save_pretrained(\"/content/drive/MyDrive/deepseek-7b-recipe-lora2\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/deepseek-7b-recipe-lora2\")\n",
        "\n",
        "print(\"\\n✅ Training complete. Model saved to: /content/drive/MyDrive/deepseek-7b-recipe-lora2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST GENERATION: USED GPU\n",
        "# ✅ STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 2: Install Unsloth\n",
        "!pip install -q unsloth\n",
        "\n",
        "# ✅ STEP 3: Load base model + fine-tuned LoRA adapter\n",
        "from unsloth import FastLanguageModel\n",
        "import torch, re\n",
        "\n",
        "base_model = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "adapter_path = \"/content/drive/MyDrive/deepseek-7b-recipe-lora2\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model,\n",
        "    max_seq_length=1024,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.load_adapter(adapter_path)\n",
        "\n",
        "# ✅ STEP 4: Test prompts\n",
        "test_prompts = [\n",
        "    \"chicken, rice, broccoli, soy sauce, garlic\",\n",
        "    \"ice cream, banana, chocolate syrup, whipped cream\",\n",
        "    \"eggs, cheese, spinach, tomato, tortilla\",\n",
        "    \"ground beef, tomato sauce, spaghetti\",\n",
        "    \"beef, potatoes, carrots, onion, broth, bay leaf\",\n",
        "    \"tofu, mushrooms, soy sauce, garlic, sesame oil\"\n",
        "]\n",
        "\n",
        "stop_phrases = [\"Note:\", \"Enjoy!\", \"submitted by\", \"Serves\", \"Source\", \"let me know\", \"Thanks for visiting\", \".com\", \"@\", \"Photo credit\"]\n",
        "\n",
        "# ✅ STEP 5: Evaluation\n",
        "def evaluate_recipe(prompt, recipe):\n",
        "    score = {\"completeness\": 0, \"creativity\": 0}\n",
        "    ingredients_list = [i.strip().lower() for i in prompt.split(\"ingredients:\")[-1].split(\".\")[0].split(\",\")]\n",
        "\n",
        "    has_ingredients = \"ingredients\" in recipe.lower()\n",
        "    has_instructions = \"instructions\" in recipe.lower()\n",
        "    used_ingredients = [i for i in ingredients_list if i in recipe.lower()]\n",
        "    coverage = len(used_ingredients) / len(ingredients_list) if ingredients_list else 0\n",
        "\n",
        "    score[\"completeness\"] = round((int(has_ingredients) + int(has_instructions) + (1 if coverage > 0.8 else 0)) / 3 * 10, 1)\n",
        "\n",
        "    verbs = re.findall(r'\\b(bake|roast|saute|simmer|boil|fry|grill|steam|whisk|marinate|blend|microwave|stir)\\b', recipe.lower())\n",
        "    adjectives = re.findall(r'\\b(crispy|creamy|savory|sweet|spicy|tender|rich|zesty|golden|charred)\\b', recipe.lower())\n",
        "\n",
        "    if len(set(verbs)) >= 3: score[\"creativity\"] += 4\n",
        "    if len(set(adjectives)) >= 2: score[\"creativity\"] += 3\n",
        "    if len(recipe.split()) > 100: score[\"creativity\"] += 3\n",
        "\n",
        "    return score\n",
        "\n",
        "# ✅ STEP 6: Run inference\n",
        "for i, ingredients in enumerate(test_prompts, 1):\n",
        "    prompt = f\"<|user|> Generate a structured recipe using ONLY these ingredients: {ingredients}. Provide just the title, ingredients list, and step-by-step instructions. <|assistant|>\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        top_k=40,\n",
        "        top_p=0.92,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.15,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    recipe = decoded.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    for phrase in stop_phrases:\n",
        "        if phrase in recipe:\n",
        "            recipe = recipe.split(phrase)[0].strip()\n",
        "\n",
        "    scores = evaluate_recipe(prompt, recipe)\n",
        "    print(f\"\\n🧪 Test Case {i}: {ingredients}\")\n",
        "    print(\"=== Generated Recipe ===\\n\", recipe)\n",
        "    print(\"📊 Completeness Score:\", scores[\"completeness\"], \"/ 10\")\n",
        "    print(\"🎨 Creativity Score:\", scores[\"creativity\"], \"/ 10\")\n",
        "    print(\"=\" * 60)\n"
      ],
      "metadata": {
        "id": "VJgsxNLDUTQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: DID NOT RUN ON CPU, GO TO NEXT BLOCK\n",
        "# ✅ STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 2: Install Unsloth\n",
        "!pip install -q unsloth\n",
        "\n",
        "# ✅ STEP 3: Load base model + fine-tuned LoRA adapter\n",
        "from unsloth import FastLanguageModel\n",
        "import torch, re\n",
        "\n",
        "base_model = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "adapter_path = \"/content/drive/MyDrive/deepseek-7b-recipe-lora2\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model,\n",
        "    max_seq_length=1024,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.load_adapter(adapter_path)\n",
        "\n",
        "# ✅ STEP 4: Get user input\n",
        "ingredients = input(\"📝 Enter ingredients (comma-separated): \").strip()\n",
        "\n",
        "stop_phrases = [\"Note:\", \"Enjoy!\", \"submitted by\", \"Serves\", \"Source\", \"let me know\", \"Thanks for visiting\", \".com\", \"@\", \"Photo credit\"]\n",
        "\n",
        "# ✅ STEP 5: Evaluation\n",
        "def evaluate_recipe(prompt, recipe):\n",
        "    score = {\"completeness\": 0, \"creativity\": 0}\n",
        "    ingredients_list = [i.strip().lower() for i in prompt.split(\"ingredients:\")[-1].split(\".\")[0].split(\",\")]\n",
        "\n",
        "    has_ingredients = \"ingredients\" in recipe.lower()\n",
        "    has_instructions = \"instructions\" in recipe.lower()\n",
        "    used_ingredients = [i for i in ingredients_list if i in recipe.lower()]\n",
        "    coverage = len(used_ingredients) / len(ingredients_list) if ingredients_list else 0\n",
        "\n",
        "    score[\"completeness\"] = round((int(has_ingredients) + int(has_instructions) + (1 if coverage > 0.8 else 0)) / 3 * 10, 1)\n",
        "\n",
        "    verbs = re.findall(r'\\b(bake|roast|saute|simmer|boil|fry|grill|steam|whisk|marinate|blend|microwave|stir)\\b', recipe.lower())\n",
        "    adjectives = re.findall(r'\\b(crispy|creamy|savory|sweet|spicy|tender|rich|zesty|golden|charred)\\b', recipe.lower())\n",
        "\n",
        "    if len(set(verbs)) >= 3: score[\"creativity\"] += 4\n",
        "    if len(set(adjectives)) >= 2: score[\"creativity\"] += 3\n",
        "    if len(recipe.split()) > 100: score[\"creativity\"] += 3\n",
        "\n",
        "    return score\n",
        "\n",
        "# ✅ STEP 6: Run inference\n",
        "prompt = f\"<|user|> Generate a structured recipe using ONLY these ingredients: {ingredients}. Provide just the title, ingredients list, and step-by-step instructions. <|assistant|>\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=True,\n",
        "    top_k=40,\n",
        "    top_p=0.92,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.15,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "recipe = decoded.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "for phrase in stop_phrases:\n",
        "    if phrase in recipe:\n",
        "        recipe = recipe.split(phrase)[0].strip()\n",
        "\n",
        "scores = evaluate_recipe(prompt, recipe)\n",
        "print(\"\\n=== Generated Recipe ===\\n\", recipe)\n",
        "print(\"📊 Completeness Score:\", scores[\"completeness\"], \"/ 10\")\n",
        "print(\"🎨 Creativity Score:\", scores[\"creativity\"], \"/ 10\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "id": "J54eQJkvt6v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: USED CPU SUCCESSFULLY\n",
        "# ✅ STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 2: Install normal libraries\n",
        "!pip install -q transformers peft\n",
        "\n",
        "# ✅ STEP 3: Load model correctly\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Paths\n",
        "base_model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "adapter_path = \"/content/drive/MyDrive/deepseek-7b-recipe-lora2\"\n",
        "\n",
        "# ✅ Load base model (CPU, float32)\n",
        "print(\"🚀 Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# ✅ Apply LoRA adapter\n",
        "print(\"🔗 Applying LoRA...\")\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"✅ Model ready!\")\n",
        "\n",
        "# ✅ STEP 4: Get user input\n",
        "ingredients = input(\"📝 Enter ingredients (comma-separated): \").strip()\n",
        "\n",
        "stop_phrases = [\"Note:\", \"Enjoy!\", \"submitted by\", \"Serves\", \"Source\", \"let me know\", \"Thanks for visiting\", \".com\", \"@\", \"Photo credit\"]\n",
        "\n",
        "# ✅ STEP 5: Evaluation function\n",
        "import re\n",
        "\n",
        "def evaluate_recipe(prompt, recipe):\n",
        "    score = {\"completeness\": 0, \"creativity\": 0}\n",
        "    ingredients_list = [i.strip().lower() for i in prompt.split(\"ingredients:\")[-1].split(\".\")[0].split(\",\")]\n",
        "\n",
        "    has_ingredients = \"ingredients\" in recipe.lower()\n",
        "    has_instructions = \"instructions\" in recipe.lower()\n",
        "    used_ingredients = [i for i in ingredients_list if i in recipe.lower()]\n",
        "    coverage = len(used_ingredients) / len(ingredients_list) if ingredients_list else 0\n",
        "\n",
        "    score[\"completeness\"] = round((int(has_ingredients) + int(has_instructions) + (1 if coverage > 0.8 else 0)) / 3 * 10, 1)\n",
        "\n",
        "    verbs = re.findall(r'\\b(bake|roast|saute|simmer|boil|fry|grill|steam|whisk|marinate|blend|microwave|stir)\\b', recipe.lower())\n",
        "    adjectives = re.findall(r'\\b(crispy|creamy|savory|sweet|spicy|tender|rich|zesty|golden|charred)\\b', recipe.lower())\n",
        "\n",
        "    if len(set(verbs)) >= 3: score[\"creativity\"] += 4\n",
        "    if len(set(adjectives)) >= 2: score[\"creativity\"] += 3\n",
        "    if len(recipe.split()) > 100: score[\"creativity\"] += 3\n",
        "\n",
        "    return score\n",
        "\n",
        "# ✅ STEP 6: Run inference\n",
        "prompt = f\"<|user|> Generate a structured recipe using ONLY these ingredients: {ingredients} The user prefe {user_pref} Allergies are {allergies}, do not. The user previously liked these recipes {liked_recipes} . Provide just the title, ingredients list, and step-by-step instructions. <|assistant|>\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=True,\n",
        "    top_k=40,\n",
        "    top_p=0.92,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.15,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "recipe = decoded.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "for phrase in stop_phrases:\n",
        "    if phrase in recipe:\n",
        "        recipe = recipe.split(phrase)[0].strip()\n",
        "\n",
        "scores = evaluate_recipe(prompt, recipe)\n",
        "\n",
        "print(\"\\n=== Generated Recipe ===\\n\", recipe)\n",
        "print(\"📊 Completeness Score:\", scores[\"completeness\"], \"/ 10\")\n",
        "print(\"🎨 Creativity Score:\", scores[\"creativity\"], \"/ 10\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "id": "1VrvJGz60izg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ATTEMPTED TO MERGE LORA AND BASE MODEL: DIDNT WORK\n",
        "# ✅ STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 2: Install Unsloth\n",
        "!pip install -q unsloth\n",
        "\n",
        "# ✅ STEP 3: Import\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# ✅ STEP 4: Load base model + adapter\n",
        "base_model_path = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "lora_adapter_path = \"/content/drive/MyDrive/deepseek-7b-recipe-lora2\"\n",
        "save_merged_path = \"/content/drive/MyDrive/deepseek-7b-merged-recipe-model\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_path,\n",
        "    max_seq_length=1024,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ✅ STEP 5: Load LoRA adapter (already patched)\n",
        "model.load_adapter(lora_adapter_path)\n",
        "\n",
        "# ✅ STEP 6: Merge adapter into base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# ✅ STEP 7: Save the final merged model\n",
        "model.save_pretrained(save_merged_path)\n",
        "tokenizer.save_pretrained(save_merged_path)\n",
        "\n",
        "print(\"\\n✅ Merge complete! Merged model saved to:\", save_merged_path)\n"
      ],
      "metadata": {
        "id": "L71L2O43qj-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}